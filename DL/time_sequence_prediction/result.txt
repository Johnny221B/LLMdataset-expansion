train.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load('/root/autodl-tmp/netthink/DL/time_sequence_prediction/traindata.pt')
STEP:  0
loss: 0.5023738120466186
loss: 0.4985663932454725
loss: 0.479011960201462
loss: 0.44633490395179287
loss: 0.3540631027261265
loss: 0.20507018018165277
loss: 1.3960544675936795
loss: 0.03249442160928485
loss: 0.029934878972933975
loss: 0.02832682311862837
loss: 0.02683061575789484
loss: 0.02377120707730244
loss: 0.018901421315100187
loss: 0.010646824595743964
loss: 0.008725753397927033
loss: 0.007872182058727188
loss: 0.00547784397325809
loss: 0.004051934991144116
loss: 0.0027296270509009316
loss: 0.0015402697323595126
test loss: 0.0013000894101894548
STEP:  1
loss: 0.0012797663945830868
loss: 0.0011690563508991606
loss: 0.0011498925364591722
loss: 0.0011288249265203648
loss: 0.0010630549289507177
loss: 0.0009563977558897171
loss: 0.0008210794177759044
loss: 0.0007670824741782826
loss: 0.000729473296039222
loss: 0.0007246574843769061
loss: 0.0007206232232254208
loss: 0.000712671248819467
loss: 0.0006961347274589921
loss: 0.0006641155429862981
loss: 0.0006101283738644877
loss: 0.000528531257224014
loss: 0.00041273422885301976
loss: 0.0003302206531783286
loss: 0.0003121690126186772
loss: 0.00032349338741264905
test loss: 0.00017005006602127817
STEP:  2
loss: 0.00030530782246219687
loss: 0.0003041505055391529
loss: 0.00030351429723221243
loss: 0.00030276621743048485
loss: 0.00030103200806123783
loss: 0.0002974385385361002
loss: 0.0002907556250033691
loss: 0.00028094065220462546
loss: 0.00026821077806359165
loss: 0.0002522027838084283
loss: 0.00023918952177290868
loss: 0.00022589596695923926
loss: 0.00022718148539581932
loss: 0.000208036252116099
loss: 0.00020052777117181303
loss: 0.00019150777052661833
loss: 0.00018778247917972734
loss: 0.0001811845627273895
loss: 0.00017654822382220389
loss: 0.00016929999892110104
test loss: 6.58138944784779e-05
STEP:  3
loss: 0.00015964540895019696
loss: 0.0001539253372923029
loss: 0.00015169599500789444
loss: 0.00015151050071665492
loss: 0.00015118914851654995
loss: 0.00015107217181042134
loss: 0.0001508618970204517
loss: 0.00015074319072558995
loss: 0.00015061697996375552
loss: 0.00015048849896457455
loss: 0.0001503107277000712
loss: 0.00015002347578924803
loss: 0.00014952091239842492
loss: 0.0001487312245019262
loss: 0.00014718181183286206
loss: 0.00014478289206238044
loss: 0.00014073822336103822
loss: 0.00013223821572606474
loss: 0.00012006753887353824
loss: 0.00010807475202117036
test loss: 4.939438352003886e-05
STEP:  4
loss: 0.00010530505466456802
loss: 9.854286865743562e-05
loss: 9.612525323348203e-05
loss: 9.211327470702697e-05
loss: 8.666054680954823e-05
loss: 8.364259347369052e-05
loss: 7.986642230781342e-05
loss: 7.815958251808212e-05
loss: 7.566097763780723e-05
loss: 7.60118727842641e-05
loss: 7.426579852735506e-05
loss: 7.395040554670093e-05
loss: 7.381224159647631e-05
loss: 7.352003290176423e-05
loss: 7.329674093720461e-05
loss: 7.277814133502677e-05
loss: 7.15668587676107e-05
loss: 6.988489908009563e-05
loss: 6.894771931503012e-05
loss: 6.905276450229448e-05
test loss: 4.151671876303376e-05
STEP:  5
loss: 6.865589496361282e-05
loss: 6.862717437292126e-05
loss: 6.852295843739041e-05
loss: 6.838518548027012e-05
loss: 6.805636869343026e-05
loss: 6.742896392279688e-05
loss: 6.660769386533554e-05
loss: 6.40760524170577e-05
loss: 6.276788741625943e-05
loss: 5.894390027062551e-05
loss: 5.465034624303055e-05
loss: 0.00022062433314407214
loss: 0.00016393960718228052
loss: 0.6454386450907346
loss: 651084.9209144303
loss: 37269.67136187602
loss: 7367021.707607132
loss: 471344593566452.75
loss: 16396950784732.832
loss: 2.820261210174e+19
test loss: 4.073382801311576e+21
STEP:  6
loss: 4.1067520728901055e+21
loss: 1.5782054502220578e+24
loss: 4.349260925286986e+23
loss: 3.5583469053394795e+27
loss: 1.1315072856909524e+26
loss: 4.447056411377824e+26
loss: 1.944516910074353e+29
loss: 1.2571086166844608e+28
loss: 4.31321833328281e+24
loss: 2.1092033561487887e+25
loss: 1.3075986265715463e+26
loss: 1.5444082641338085e+26
loss: 5.216803026132398e+27
loss: 1.0220730371573296e+27
loss: 1.5343731386369962e+26
loss: 2.7188205590942094e+27
loss: 3.889403539790479e+26
loss: 7.59565189931655e+26
loss: 3.4697737841507106e+28
loss: 5.497559404828693e+27
test loss: 1.17653589649791e+27
STEP:  7
loss: 1.37831256184494e+27
loss: 4.539238716622299e+26
loss: 6.797416926062562e+24
loss: 2.4892606252615163e+26
loss: 8.069854259295447e+24
loss: 7.129695171664048e+24
loss: 5.581280564605552e+24
loss: 6.522029873541315e+24
loss: 1.0342595572534425e+25
loss: 2.81411553675396e+25
loss: 3.941804599639212e+26
loss: 3.8519902446235196e+26
loss: 1.217120965885772e+27
loss: 3.854094025909898e+26
loss: 3.983904133680249e+26
loss: 2.183963193625762e+26
loss: 9.883298227381613e+25
loss: 7.612869895400131e+25
loss: 7.063713720198087e+25
loss: 4.121051025922816e+26
test loss: 2.0106943553105688e+26
STEP:  8
loss: 1.4230404880584364e+26
loss: 9.529829902551554e+25
loss: 7.703698742553736e+25
loss: 8.32217723962465e+25
loss: 4.0073619843750975e+25
loss: 3.6198029679931347e+25
loss: 1.7804080775758646e+26
loss: 3.800223002660873e+25
loss: 2.4197531765197075e+25
loss: 2.380382219516056e+25
loss: 6.19674724867426e+26
loss: 1.4260842676852185e+28
loss: 1.8431298851939175e+26
loss: 1.5536213459121132e+26
loss: 2.3074590287584386e+27
loss: 4.021981687338983e+29
loss: 2.249923765074525e+28
loss: 3.7595972138796805e+26
loss: 7.275276883597623e+26
loss: 6.954847945609057e+25
test loss: 8.05201615737579e+25
STEP:  9
loss: 9.861423883734496e+25
loss: 2.659173999788249e+25
loss: 2.1892593826204173e+25
loss: 1.94905277711528e+25
loss: 1.8877547048785335e+25
loss: 4.7709155020844374e+26
loss: 2.0067374327431043e+26
loss: 1.8829031227261437e+26
loss: 1.39795441327738e+25
loss: 6.758024585524057e+24
loss: 1.07274334155586e+25
loss: 9.119825994323657e+24
loss: 1.1867989718548879e+25
loss: 1.7714447109857853e+25
loss: 3.74865796459436e+25
loss: 8.231161000268415e+25
loss: 3.434499950057889e+25
loss: 1.4477109440567088e+25
loss: 1.1895015934300834e+25
loss: 7.729793016391486e+24
test loss: 4.3318094214526665e+26
STEP:  10
loss: 4.117923148070162e+26
loss: 6.96537042112163e+29
loss: 2.8486531585415145e+28
loss: 1.7019331830774782e+27
loss: 1.9732220249474985e+27
loss: 1.6135315088807961e+28
loss: 3.0340276001825374e+27
loss: 1.8484061989979883e+27
loss: 1.7950510574798685e+27
loss: 2.1557889290819927e+27
loss: 1.0981512597772486e+27
loss: 1.976649394280742e+27
loss: 3.2353896333694284e+26
loss: 1.1425192555969305e+27
loss: 1.2750387367112431e+27
loss: 4.5728553579908095e+25
loss: 1.2105217325728827e+26
loss: 5.6079581605317925e+25
loss: 5.05416769239589e+25
loss: 4.761947024271803e+25
test loss: 5.677829119563354e+26
STEP:  11
loss: 5.7113300575320695e+26
loss: 8.717581468325707e+26
loss: 3.3528622653825125e+28
loss: 5.296316012342568e+27
loss: 1.6310116235795114e+27
loss: 3.144425405725014e+26
loss: 1.833272052291717e+26
loss: 1.2968247234899058e+26
loss: 9.255055632258707e+25
loss: 7.39051870930694e+27
loss: 6.777489015738199e+25
loss: 6.4852189264628355e+25
loss: 2.033856097709809e+26
loss: 1.7981865958787774e+25
loss: 2.885531468503579e+25
loss: 2.3446555152517823e+25
loss: 8.359895335102651e+25
loss: 2.330624461942029e+26
loss: 6.272518177888823e+27
loss: 7.859176631646511e+26
test loss: 4.735707319665751e+27
STEP:  12
loss: 4.719197928718447e+27
loss: 7.320409361977969e+26
loss: 2.875915472938132e+26
loss: 1.1455082608250208e+26
loss: 1.1458058174800755e+26
loss: 9.760348184096251e+28
loss: 4.0679471952289335e+26
loss: 2.113809580720726e+26
loss: 1.6332481506687097e+26
loss: 2.358044669121111e+25
loss: 2.075562192337558e+25
loss: 1.6129241002766756e+25
loss: 1.877389787529172e+25
loss: 5.0885597972619555e+26
loss: 5.108850994517711e+25
loss: 2.0231632370191443e+26
loss: 1.103517302152394e+26
loss: 7.944969740874e+25
loss: 1.3821366119581346e+26
loss: 1.4756186282095684e+26
test loss: 3.421682813312008e+26
STEP:  13
loss: 3.639507813784127e+26
loss: 4.7145822324047645e+26
loss: 2.3228505834890587e+26
loss: 3.603497704082969e+27
loss: 3.1231105725403563e+27
loss: 3.120924128735919e+29
loss: 1.9570144794917197e+29
loss: 2.510652432691356e+32
loss: 6.232409679966546e+30
loss: 1.7080908985982825e+29
loss: 5.349540986520324e+28
loss: 5.205773428059739e+28
loss: 1.556222465419511e+29
loss: 7.376770315290308e+28
loss: 3.4269970371221385e+28
loss: 3.5462001572956324e+28
loss: 2.6944300405630455e+28
loss: 2.1159306288637575e+28
loss: 6.826741542305822e+28
loss: 1.9937770464327206e+30
test loss: 2.5487958870123714e+28
STEP:  14
loss: 3.2223289526434575e+28
loss: 1.0435203625546573e+28
loss: 4.288301815714348e+27
loss: 3.4529631904394606e+27
loss: 2.783594377137902e+27
loss: 2.4910246014020325e+27
loss: 1.1587397962988021e+27
loss: 8.610334473106844e+26
loss: 5.48389278057387e+26
loss: 4.997328632556006e+26
loss: 4.4452482327387375e+26
loss: 3.696089941029144e+26
loss: 2.762115777676322e+26
loss: 2.3855564798682168e+26
loss: 2.367389886256474e+26
loss: 2.3388762723984784e+26
loss: 2.2476451318526587e+26
loss: 1.7976859193327913e+26
loss: 2.3605585681448634e+27
loss: 1.6519357874137095e+30
test loss: 3.4062877865788945e+29
